{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9eba3a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw03.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945df65c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "#### Homework 3 Supplemental Notebook\n",
    "    \n",
    "# Vectors and the Dot Product\n",
    "\n",
    "### EECS 245, Winter 2026 at the University of Michigan\n",
    "    \n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Most homeworks will have Jupyter Notebooks, like this one, designed to supplement the theoretical problems. \n",
    "\n",
    "To write and run code in this notebook, you have two options:\n",
    "\n",
    "1. **Set up a Jupyter Notebook environment locally, and use `git` to clone our course repository (preferred).** For instructions on how to do this, see the [**Environment Setup**](https://eecs245.org/env-setup) page of the course website.\n",
    "1. **Use the EECS 245 DataHub.** To do this, click the link provided in the Homework 3 PDF. Before doing so, read the instructions on the [**Environment Setup**](https://eecs245.org/env-setup/#option-2-using-the-eecs-245-datahub) page on how to use the DataHub.\n",
    "\n",
    "To receive credit for the programming portion of the homework, you'll need to submit your completed notebook to the autograder on Gradescope. Your submission time for Homework 3 is the **latter** of your PDF and code submission times.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    Since this problem is brand-new this semester, there are no hidden test cases, just in case there are bugs in the code. The tests in your notebook are the same as the test cases on Gradescope, and when you submit to Gradescope, you should be able to see your final score.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import util\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb792e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 6: Neighbors üè° (10 pts)\n",
    "\n",
    "---\n",
    "\n",
    "So far in EECS 245, we've focused on **regression** problems, where the goal is to predict a continuous value, e.g. commute time. In this problem, we'll explore a **classification** problem, where the goal is to predict a categorical value. Examples of classification problems include:\n",
    "- Does this person have diabetes?\n",
    "- Is this digit a 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9? (We saw this in [Chapter 1.1](https://notes.eecs245.org/introduction-to-supervised-learning/what-is-machine-learning/)).\n",
    "- Is this picture of a dog, cat, zebra, or hamster?\n",
    "\n",
    "We'll explore the first problem here: predicting whether or not a patient has diabetes, given their `Glucose` and `BMI` levels. Let's start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfd2e6",
   "metadata": {},
   "source": [
    "`Glucose` is measured in mg/dL (milligrams per deciliter); `BMI` is calculated as $\\text{BMI} = \\frac{\\text{weight (kg)}}{\\left[ \\text{height (m)} \\right]^2}$. In the `Outcome` column, `0` means no diabetes, `1` means yes diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ac6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64.9% of individuals in the dataset do not have diabetes.\n",
    "diabetes['Outcome'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc16fc",
   "metadata": {},
   "source": [
    "Before we visualize the data, we'll split the dataset into training and test sets. The training set will be used to fit the model, and the test set will be used to evaluate the model on \"unseen\" data. This is an idea we'll revisit in future homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246b164",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Above, `X_train` is a DataFrame (technically not a `numpy` array) containing a random sample of 75% of the rows from the full dataset, and just the `Glucose` and `BMI` columns. We will not work directly with DataFrames: instead, the functions you implement below will take in `numpy` arrays. We'll handle the DataFrame-to-array conversion for you once it's necessary.\n",
    "\n",
    "Below, <span style='color: orange'><b>class 0 (orange) is \"no diabetes\"</b></span> and <span style='color: blue'><b>class 1 (blue) is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90949c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = util.create_base_scatter(X_train, y_train)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1be437",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using this dataset, how can we classify whether someone new (not already in the dataset) has diabetes, given their `Glucose` and `BMI`?\n",
    "\n",
    "**Here's the intuition behind the $k$-nearest neighbors üè° classifier**: if a new person's feature vector is <b><span style=\"color:blue\">close to the blue points</span></b>, we'll predict <b><span style=\"color:blue\">blue (diabetes)</span></b>; if they're <b><span style=\"color:orange\">close to the orange points</span></b>, we'll predict <b><span style=\"color:orange\">orange (no diabetes)</span></b>.\n",
    "\n",
    "That is, suppose we're given a new individual, $\\vec{x}_\\text{new} = \\begin{bmatrix} \\text{Glucose}_\\text{new} \\\\ \\text{BMI}_\\text{new} \\end{bmatrix}$. The $k$-nearest neighbors classifier ($k$-NN for short) classifies $\\vec{x}_\\text{new}$ by:\n",
    "1. Finding the $k$ **closest points** in the training set to $\\vec{x}_\\text{new}$.\n",
    "1. Predicting that $\\vec{x}_\\text{new}$ belongs to the **most common class** among those $k$ closest points.\n",
    "\n",
    "For example, suppose $k = 6$. If, among the 6 closest points to $\\vec{x}_\\text{new}$, there are <b><span style=\"color:blue\">4 blue</span></b> and <b><span style=\"color:orange\">2 orange</span></b> points, we'd predict <b><span style=\"color:blue\">blue (diabetes)</span></b>. If there are ties, we'll have to break them somehow; read [here](https://stats.stackexchange.com/questions/144718/how-does-scikit-learn-resolve-ties-in-the-knn-classification) for more information.\n",
    "\n",
    "Some questions you may have:\n",
    "- How do we measure \"closeness\" between two points? Suppose $\\vec x_i$ and $\\vec x_j$ are two vectors in $\\mathbb{R}^n$. To measure how close they are, we can compute the **norm (i.e. length) of the difference between the two vectors**. (Refer to [Chapter 3.2](https://notes.eecs245.org/vectors/norms/) for a discussion of norms.) There are multiple vector norms, which means we have a choice to make:\n",
    "    $$\\text{using the $L_2$ norm: } \\text{dist}(\\vec x_i, \\vec x_j) = \\|\\vec x_i - \\vec x_j\\|_2 = \\sqrt{\\sum_{d=1}^n (\\text{component $d$ of } \\vec x_i - \\text{component $d$ of } \\vec x_j)^2}$$\n",
    "    $$\\text{using the $L_1$ norm: } \\text{dist}(\\vec x_i, \\vec x_j) = \\|\\vec x_i - \\vec x_j\\|_1 = \\sum_{d=1}^n |\\text{component $d$ of } \\vec x_i - \\text{component $d$ of } \\vec x_j|$$\n",
    "    For instance, if $\\vec x_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ and $\\vec x_2 = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}$, then $\\|\\vec x_1 - \\vec x_2\\|_2 = 5$ and $\\|\\vec x_1 - \\vec x_2\\|_1 = 7$.\n",
    "- What value of $k$ should we use, i.e., how many neighbors should we look at? The value of $k$ is called a **hyperparameter**, because it's a parameter of the model that we get to choose (unlike, say, the $w_0$'s and $w_1$'s in simple linear regression, which are found through minimizing mean squared error, not by us picking them). Soon, we'll see how different values of $k$ can lead to different models.\n",
    "\n",
    "This problem will walk you through implementing the $k$-nearest neighbors classifier. By the end, you'll implement a `KNNClassifier` class (similar to the `SimpleLAD` class from Homework 2) that you can use to classify new points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0fe09b",
   "metadata": {},
   "source": [
    "### Problem 6a) (2 pts)\n",
    "\n",
    "Complete the implementation of `lp_distance`, which takes in:\n",
    "- `x_i` and `x_j`, two 1D `numpy` arrays with the same number of components, and\n",
    "- `p`, an integer equal to either `1` or `2`, corresponding to the $L_1$ or $L_2$ norm.\n",
    "\n",
    "`lp_distance` should return a **float** representing $\\|\\vec x_i-\\vec x_j\\|_p$. That is, if `p = 1`, it should return the $L_1$ norm of $\\vec x_i - \\vec x_j$; if `p = 2`, it should return the $L_2$ norm of $\\vec x_i - \\vec x_j$. Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> lp_distance(np.array([1, 2]), np.array([4, 6]), 1)\n",
    "7.0\n",
    "\n",
    ">>> lp_distance(np.array([1, 2]), np.array([4, 6]), 2)\n",
    "5.0\n",
    "```\n",
    "\n",
    "**Don't** use a `for`-loop, and also don't use `np.linalg.norm`. Instead, use `np.abs`, `np.sum`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aea534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lp_distance(x_i, x_j, p):\n",
    "    if p not in [1, 2]:\n",
    "        raise ValueError('p must be 1 or 2')\n",
    "        \n",
    "    if x_i.shape != x_j.shape:\n",
    "        raise ValueError('x_i and x_j must have the same shape')\n",
    "\n",
    "    ...\n",
    "\n",
    "# Feel free to change these inputs to test your function.\n",
    "print(lp_distance(np.array([1, 2]), np.array([4, 6]), 1))\n",
    "print(lp_distance(np.array([1, 2]), np.array([4, 6]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b1919",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p06_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd4d58",
   "metadata": {},
   "source": [
    "### Problem 6b) (2 pts)\n",
    "\n",
    "Complete the implementation of `all_distances`, which takes in:\n",
    "- `X_train`, a 2D `numpy` array with shape `(n, d)`,\n",
    "- `x_new`, a 1D `numpy` array with `d` components, and\n",
    "- `p`, an integer equal to either `1` or `2`, corresponding to the $L_1$ or $L_2$ norm.\n",
    "\n",
    "`all_distances` should return a 1D `numpy` array of length `n` containing distances from `x_new` to each row of `X_train`. Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> X_demo = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [6, 10]\n",
    "])\n",
    ">>> all_distances(X_demo, np.array([1, 1]), 1)\n",
    "array([1.0, 3.0, 5.0, 14.0])\n",
    "\n",
    ">>> all_distances(X_demo, np.array([1, 1]), 2)\n",
    "array([1.0, 2.24, 3.61, 10.3])\n",
    "```\n",
    "\n",
    "You **can** use a `for`-loop, though this can be done without one. To access row `i` of a `numpy` array `X`, you can use `X[i]`. Column `j` (if you need it) can be accessed with `X[:, j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2f48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def all_distances(X_train, x_new, p):\n",
    "    \"\"\"Return a 1D array of distances from x_new to each row of X_train.\"\"\"\n",
    "    ...\n",
    "\n",
    "# Feel free to change these inputs to test your function.\n",
    "X_demo = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [6, 10]\n",
    "])\n",
    "print(all_distances(X_demo, np.array([1, 1]), 1))\n",
    "print(all_distances(X_demo, np.array([1, 1]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537b755",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p06_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078538f",
   "metadata": {},
   "source": [
    "Great work so far. We're going to handle the next step for you. Below, we've implemented the function `knn_indices`, which takes in:\n",
    "- `X_train`, a 2D `numpy` array with shape `(n, d)`,\n",
    "- `x_new`, a 1D `numpy` array with `d` components,\n",
    "- `k`, the number of nearest neighbors to find in the algorithm, and\n",
    "- `p`, an integer equal to either `1` or `2`, corresponding to the $L_1$ or $L_2$ norm.\n",
    "\n",
    "`knn_indices` returns a 1D array of length `k` (not `n` or `d`!) containing the integer indices of the `k` nearest neighbors of `x_new` in `X_train`. It does this by calling your `all_distances` function on `X_train` and `x_new`, and then using `np.argsort` to find the indices of the smallest `k` values in the distance array. We use mergesort's handling of ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d56147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_indices(X_train, x_new, k, p):\n",
    "    dists = all_distances(X_train, x_new, p)\n",
    "    return np.argsort(dists, kind='mergesort')[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea4bd3",
   "metadata": {},
   "source": [
    "Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_demo_shuffled = np.array([\n",
    "    [3, 4],\n",
    "    [2, 3],\n",
    "    [6, 10],\n",
    "    [1, 2]\n",
    "])\n",
    "knn_indices(X_demo_shuffled, np.array([1, 1]), k=3, p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d79d44",
   "metadata": {},
   "source": [
    "**If you've implemented everything correctly so far**, the above should show `np.array([3, 1, 0])`. This is because the 3rd row of `X_demo` is closest to `x_new` in the $L_2$ norm, the 1st row is the second closest, and the 0th row is the third closest.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Make sure you understand how the call above works before moving forward!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f59478",
   "metadata": {},
   "source": [
    "### Problem 6c) (2 pts)\n",
    "\n",
    "So far, we've only dealt with the input variables, `Glucose` and `BMI`. Now, let's stitch our work together to actually classify whether someone has diabetes.\n",
    "\n",
    "Complete the implementation of `knn_predict`, which takes in:\n",
    "- `X_train`, a 2D `numpy` array with shape `(n, d)`,\n",
    "- `y_train`, a 1D `numpy` array with `d` components, containing the values `1` (yes diabetes) or `0` (no diabetes),\n",
    "- `x_new`, a 1D `numpy` array with `d` components,\n",
    "- `k`, the number of nearest neighbors to find in the algorithm, and\n",
    "- `p`, an integer equal to either `1` or `2`, corresponding to the $L_1$ or $L_2$ norm.\n",
    "\n",
    "`knn_predict` should return a `1` or `0` representing the predicted class label for `x_new`, by finding the most common class (1 or 0) among the `k` nearest neighbors of `x_new` in `X_train`. Use the `knn_indices` function you implemented above to find the neighbors. If `k` is even and there are an equal number of 1s and 0s among the nearest neighbors, you should return `1`.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> X_demo = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [6, 10]\n",
    "])\n",
    ">>> y_demo = np.array([0, 0, 1, 1])\n",
    ">>> knn_predict(X_demo, y_demo, np.array([1, 1]), k=3, p=2)\n",
    "0 # Make sure you understand where this came from!\n",
    "```\n",
    "\n",
    "**Don't** use a `for`-loop. Once you have the indices of the nearest neighbors, you can use them to index into `y_train` to get the labels of the nearest neighbors:\n",
    "\n",
    "```python\n",
    ">>> idx = np.array([3, 5, 1])\n",
    ">>> y[idx] # This is valid syntax, and will return just the elements of y at positions 3, 5, and 1.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c2eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def knn_predict(X_train, y_train, x_new, k, p):\n",
    "    ...\n",
    "\n",
    "# Feel free to change these inputs to test your function.\n",
    "# In particular, test it on values of k that are larger than 2 or 3!\n",
    "X_demo = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [6, 10]\n",
    "])\n",
    "y_demo = np.array([0, 0, 1, 1])\n",
    "knn_predict(X_demo, y_demo, np.array([1, 1]), k=3, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b7b7f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p06_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f073d9",
   "metadata": {},
   "source": [
    "### Problem 6d) (2 pts)\n",
    "\n",
    "Finally, complete the implementation of the `KNNClassifier` class, which has two methods, apart from the constructor.\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "All `fit` should do is store the training data in the instance variables `self.X_train_` and `self.y_train_`. Unlike `SimpleLAD` in Homework 2, there are no parameters to solve for, so `fit` doesn't really need to do anything else. ($k$-NN is called a **non-parametric method**.)\n",
    "\n",
    "#### predict\n",
    "\n",
    "`predict` takes in a single (non-`self`) input, named `x_new`.\n",
    "- If `x_new` is a 1D array, `predict` should return a single prediction (either `1` or `0`) as an integer.\n",
    "- If `x_new` is a 2D array, `predict` should return a 1D array of predictions, one for each row in `x_new`. **You can `for`-loop in this case.**\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> X_small = np.array([\n",
    "    [148, 33.6],\n",
    "    [85, 26.6],\n",
    "    [183, 23.3],\n",
    "    [89, 28.1],\n",
    "    [137, 43.1],\n",
    "    [116, 25.6],\n",
    "])\n",
    ">>> y_small = np.array([1, 0, 0, 1, 1, 0])\n",
    ">>> model = KNNClassifier(k=5, p=2)\n",
    ">>> model.fit(X_small, y_small) # No output is expected!\n",
    "\n",
    ">>> x_new = np.array([110, 30.0])\n",
    ">>> model.predict(x_new)\n",
    "0\n",
    "\n",
    ">>> x_new_multiple = np.array([\n",
    "    [110, 30.0],\n",
    "    [148, 33.6],\n",
    "    [85, 26.6],\n",
    "])\n",
    ">>> model.predict(x_new_multiple)\n",
    "array([0, 1, 0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2106d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k=3, p=2):\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        if len(X_train) != len(y_train):\n",
    "            raise ValueError(f'Dimension mismatch: X has length {len(X_train)} while y has length {len(y_train)}')\n",
    "\n",
    "        if np.unique(y_train).size != 2:\n",
    "            raise ValueError('y_train must contain exactly two classes.')\n",
    "\n",
    "        # Your solution only needs to be two lines of code.\n",
    "        # They are very short; we're leaving them as a task for you just to make sure you understand the code.\n",
    "        ...\n",
    "\n",
    "    # Hint: You'll find yourself using many instance variables, like\n",
    "    # self.X_train_ and self.p, in your implementation.\n",
    "    def predict(self, x_new):\n",
    "        if isinstance(x_new, list):\n",
    "            x_new = np.array(x_new)\n",
    "        try:\n",
    "            # If x_new is a single point:\n",
    "            if x_new.ndim == 1:\n",
    "                ...\n",
    "            # Otherwise, x_new is a 2D array:\n",
    "            else:\n",
    "                ...\n",
    "        except AttributeError:\n",
    "            raise AttributeError('Cannot use `predict` before `fit`.')\n",
    "\n",
    "# Feel free to change these inputs to test your class implementation.\n",
    "X_small = np.array([\n",
    "    [148, 33.6],\n",
    "    [85, 26.6],\n",
    "    [183, 23.3],\n",
    "    [89, 28.1],\n",
    "    [137, 43.1],\n",
    "    [116, 25.6],\n",
    "])\n",
    "y_small = np.array([1, 0, 0, 1, 1, 0])\n",
    "\n",
    "model = KNNClassifier(k=1, p=2)\n",
    "model.fit(X_small, y_small)\n",
    "\n",
    "x_new = np.array([110, 30.0])\n",
    "print(model.predict(x_new))\n",
    "\n",
    "x_new_multiple = np.array([\n",
    "    [110, 30.0],\n",
    "    [148, 33.6],\n",
    "    [85, 26.6],\n",
    "])\n",
    "\n",
    "print(model.predict(x_new_multiple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadf851",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p06_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93514d9c",
   "metadata": {},
   "source": [
    "Awesome work! Now, it's time to enjoy the fruits of your labor. Like we did at the start of the notebook, we'll load in the full dataset and divide it into training and test sets. (We're redoing this here in case we overrode the original `X_train` and `y_train` variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")\n",
    "X_train, y_train = X_train.to_numpy(), y_train.to_numpy()\n",
    "X_test, y_test = X_test.to_numpy(), y_test.to_numpy()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8394814",
   "metadata": {},
   "source": [
    "Now, we'll fit an instance on **just** the training data. For now, we'll arbitrarily use `k=5` and `p=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7329727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNNClassifier(k=5, p=2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40e226",
   "metadata": {},
   "source": [
    "One of the first metrics we'll check when training a classifier on a training set is its **training set accuracy** and **test set accuracy**. The accuracy of a model is the fraction of predictions that match the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400be091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each point in the training set, check if the model's prediction matches the true label.\n",
    "model.predict(X_train) == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set accuracy\n",
    "(model.predict(X_train) == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0542d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set accuracy\n",
    "(model.predict(X_test) == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a880ee",
   "metadata": {},
   "source": [
    "So, our model correctly classifies 81.5% of the training data and 74.4% of the test data.\n",
    "\n",
    "What does it **look like**? Let's plot the **decision boundary** of our model. Run the cell below; it may take a few seconds to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b096ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model, X_train, y_train, title='k-NN Decision Boundary (k=5, p=2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc180c",
   "metadata": {},
   "source": [
    "Regions colored <span style=\"color:blue\"><b>blue</b></span> are regions where the model predicts a label of 1 (**diabetes**), and regions colored <span style=\"color:orange\"><b>orange</b></span> are regions where the model predicts a label of 0 (**no diabetes**). So, if some new patient $\\vec x_\\text{new} = \\begin{bmatrix} \\text{Glucose}_\\text{new} \\\\ \\text{BMI}_\\text{new} \\end{bmatrix}$ comes along, all we need to do is check which region it falls into and we've made our prediction! The points shown come from the training set; the test set is not visualized at all.\n",
    "\n",
    "The decision boundary we ended up with depends on our choices of `k` (number of neighbors) and `p` (distance metric). Let's see how these choices affect the decision boundary. First, let's try changing `p` to `1` and looking at how the decision boundary changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4228f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p1 = KNNClassifier(k=5, p=1)\n",
    "model_p1.fit(X_train, y_train)\n",
    "util.show_decision_boundary(model_p1, X_train, y_train, title='k-NN Decision Boundary (k=5, p=1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623ecbd",
   "metadata": {},
   "source": [
    "The decision boundary mostly looks the same as when `p=2`, but there are some subtle differences, particularly in the top right. Think about why this is.\n",
    "\n",
    "A more pronounced difference is what happens when we change `k`. Below, you'll find a slider that allows you to change the value of `k` and see the effect on the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imgs/knn_slider.html') as f:\n",
    "    display(HTML(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29926c",
   "metadata": {},
   "source": [
    "Above, you should notice that as the value of `k` increases, the decision boundary becomes more smooth and less jagged. Very small values of `k` result in the model **overfitting** to the training data. After all, the closest neighbor to a point in the training set is the point itself, so using a low value of `k` results in the model effectively memorizing the training data. This is unlikely to generalize well to the test set. On the other hand, very large values of `k` result in a very simple model that may not be sophisticated enough to capture the complexity of the data.\n",
    "\n",
    "Which value of `k` should we use, then? One idea is to test out all 50 of the models above ‚Äì `k=1`, `k=2`, ..., `k=50` ‚Äì on the test set, picking the value of `k` with the **highest test set accuracy**. Let's do this below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff2834",
   "metadata": {},
   "source": [
    "### Problem 6e) (2 pts)\n",
    "\n",
    "In the space provided, train 50 models, one for each value of `k` from `k=1` to `k=50`. \n",
    "- Each one should be trained on `X_train` and `y_train` with `p=2`.\n",
    "- For each model, calculate the **test set accuracy** of the model. We did an example for you above.\n",
    "- Finally, assign `best_k` to the value of `k` with the **highest test set accuracy**.\n",
    "\n",
    "All we are autograding is the value of `best_k`, but you'll need to write a few lines of code to find it. Don't manually set the value of `best_k`: once you've created a list/array of test accuracies, use `np.argmax` to find the value of `k` with the highest accuracy (and make sure to add 1 to the result, since `np.argmax` returns 0-based indices). It turns out that there are multiple values of `k` that result in the same highest test set accuracy; you just need to find any one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dab35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14af9e7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p06_e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2211c7",
   "metadata": {},
   "source": [
    "Think about what would happen if we let `k` get even larger, like `k=100` or `k=200`! (How does this relate to the constant model from the first week of the semester?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43456b88",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Finish Line üèÅ\n",
    "\n",
    "Congratulations! You're ready to submit the programming portion of Homework 3.\n",
    "\n",
    "To submit your work to Gradescope:\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells.\n",
    "2. Read through the notebook to make sure everything is fine and all public tests passed.\n",
    "3. Run the cell below to run all tests, and make sure that they all pass.\n",
    "4. Download your notebook using `File -> Download`, then upload your notebook to Gradescope under \"Homework 3, Problem 6 Code\".\n",
    "5. Stick around for a few minutes while the Gradescope autograder grades your work. **Remember that this homework has no hidden tests, so you should be able to see your score shortly after submitting.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "otter": {
   "tests": {
    "p06_a": {
     "name": "p06_a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = lp_distance(np.array([1, 2]), np.array([4, 6]), 1)\n>>> np.isclose(out, 7.0)\nTrue",
         "failure_message": "lp_distance([1,2],[4,6],1) should be 7.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = lp_distance(np.array([1, 2]), np.array([4, 6]), 2)\n>>> np.isclose(out, 5.0)\nTrue",
         "failure_message": "lp_distance([1,2],[4,6],2) should be 5.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = lp_distance(np.array([-1.5, 2.0, 3.0]), np.array([2.5, -2.0, 3.0]), 1)\n>>> np.isclose(out, 8.0)\nTrue",
         "failure_message": "lp_distance returned the wrong value for floats.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = lp_distance(np.array([-1.5, 2.0, 3.0]), np.array([2.5, -2.0, 3.0]), 2)\n>>> np.isclose(out, 5.6568)\nTrue",
         "failure_message": "lp_distance returned the wrong value for floats.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> arr1 = np.arange(1, 245, 7) + 0.03\n>>> arr2 = np.arange(1, 245, 7) * 2 - 3 * np.arange(len(arr1))\n>>> out = lp_distance(arr1, arr2, 1)\n>>> np.isclose(out, 2413.95)\nTrue",
         "failure_message": "lp_distance returned the wrong value for large input.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> arr1 = np.arange(1, 245, 7) + 0.03\n>>> arr2 = np.arange(1, 245, 7) * 2 - 3 * np.arange(len(arr1))\n>>> out = lp_distance(arr1, arr2, 2)\n>>> np.isclose(out, 472.8743294999211)\nTrue",
         "failure_message": "lp_distance returned the wrong value for large input.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p06_b": {
     "name": "p06_b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = np.array([[0, 0], [1, 1], [2, 2]])\n>>> out = all_distances(X, np.array([1, 0]), 1)\n>>> np.allclose(out, np.array([1.0, 1.0, 3.0]))\nTrue",
         "failure_message": "all_distances with p=1 returned the wrong values.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[0, 0], [1, 1], [2, 2]])\n>>> out = all_distances(X, np.array([1, 0]), 2)\n>>> np.allclose(out, np.array([1.0, 1.0, np.sqrt(5.0)]))\nTrue",
         "failure_message": "all_distances with p=2 returned the wrong values.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_demo = np.array([[1, 2], [2, 3], [3, 4], [6, 10]])\n>>> x_new = np.array([1, 1])\n>>> out1 = all_distances(X_demo, x_new, 1)\n>>> out2 = all_distances(X_demo, x_new, 2)\n>>> expected1 = np.array([1.0, 3.0, 5.0, 14.0])\n>>> expected2 = np.array([1.0, np.sqrt(5.0), np.sqrt(13.0), np.sqrt(106.0)])\n>>> np.allclose(out1, expected1) and np.allclose(out2, expected2)\nTrue",
         "failure_message": "all_distances did not match the X_demo example (p=1 and/or p=2).",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> arr1 = np.arange(1, 245, 7) + 0.03\n>>> arr2 = np.arange(2, 246, 7) - 0.5\n>>> X = np.stack([arr1, arr2], axis=1)\n>>> x_new = np.array([1.2, -3.4])\n>>> \n>>> out1 = all_distances(X, x_new, 1)\n>>> expected1 = np.sum(np.abs(X - x_new), axis=1)\n>>> \n>>> out2 = all_distances(X, x_new, 2)\n>>> expected2 = np.sqrt(np.sum((X - x_new) ** 2, axis=1))\n>>> \n>>> np.allclose(out1, expected1) and np.allclose(out2, expected2)\nTrue",
         "failure_message": "all_distances returned the wrong values on a larger input (compared to a vectorized numpy reference).",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p06_c": {
     "name": "p06_c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_demo = np.array([[1, 2], [2, 3], [3, 4], [6, 10]])\n>>> y_demo = np.array([0, 0, 1, 1])\n>>> out = knn_predict(X_demo, y_demo, np.array([1, 1]), k=3, p=2)\n>>> np.isclose(out, 0)\nTrue",
         "failure_message": "knn_predict returned the wrong prediction.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n>>> y = np.array([0, 1, 0])\n>>> out = knn_predict(X_train=X, y_train=y, x_new=np.array([1, 1]), k=2, p=2)\n>>> np.isclose(out, 1)\nTrue",
         "failure_message": "knn_predict returned the wrong prediction.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n>>> y = np.array([0, 1, 0])\n>>> out = knn_predict(X_train=X, y_train=y, x_new=np.array([1, 1]), k=3, p=2)\n>>> np.isclose(out, 0)\nTrue",
         "failure_message": "knn_predict returned the wrong prediction.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> arr1 = np.arange(1, 245, 7) + 0.03\n>>> arr2 = np.arange(2, 246, 7) - 0.5\n>>> X = np.stack([arr1, arr2], axis=1)\n>>> y = np.array([0] * (arr1.shape[0] // 2) + [1] * (arr1.shape[0] // 2))\n>>> x_new = np.array([1.2, -3.4])\n>>> out = knn_predict(X_train=X, y_train=y, x_new=x_new, k=3, p=2)\n>>> np.isclose(out, 0)\nTrue",
         "failure_message": "knn_predict returned the wrong prediction.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> arr1 = np.arange(1, 245, 7) + 0.03\n>>> arr2 = np.arange(2, 246, 7) - 0.5\n>>> X = np.stack([arr1, arr2], axis=1)\n>>> y = np.array([1] * (arr1.shape[0] // 2) + [0] * (arr1.shape[0] // 2))\n>>> x_new = np.array([1.2, -3.4])\n>>> out = knn_predict(X_train=X, y_train=y, x_new=x_new, k=17, p=1)\n>>> np.isclose(out, 1)\nTrue",
         "failure_message": "knn_predict returned the wrong prediction.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> df = pd.read_csv('data/diabetes.csv')\n>>> X = df[['Glucose', 'BMI']].to_numpy()\n>>> y = df['Outcome'].to_numpy()\n>>> x_new = X[3]\n>>> out = knn_predict(X_train=X, y_train=y, x_new=x_new, k=5, p=2)\n>>> np.isclose(out, 0)\nTrue",
         "failure_message": "knn_predict returned the wrong prediction.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p06_d": {
     "name": "p06_d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = np.array([[0, 0], [2, 0], [1, 1]])\n>>> y = np.array([0, 0, 1])\n>>> model = KNNClassifier(k=2, p=2)\n>>> model.fit(X, y)\n>>> np.isclose(model.predict(np.array([1, 0])), 0)\nTrue",
         "failure_message": "KNNClassifier.predict returned the wrong label for a single point.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[0, 0], [2, 0], [1, 1]])\n>>> y = np.array([0, 0, 1])\n>>> model = KNNClassifier(k=1, p=2)\n>>> model.fit(X, y)\n>>> out = model.predict(np.array([[0, 0], [1, 1]]))\n>>> np.array_equal(out, np.array([0, 1]))\nTrue",
         "failure_message": "KNNClassifier.predict returned the wrong labels for multiple points.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_small = np.array([[148, 33.6], [85, 26.6], [183, 23.3], [89, 28.1], [137, 43.1], [116, 25.6]])\n>>> y_small = np.array([1, 0, 0, 1, 1, 0])\n>>> \n>>> model = KNNClassifier(k=1, p=2)\n>>> model.fit(X_small, y_small)\n>>> \n>>> x_new = np.array([110, 30.0])\n>>> np.isclose(model.predict(x_new), 0)\nTrue",
         "failure_message": "KNNClassifier.predict returned the wrong labels for a single point (example input from the prompt).",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_small = np.array([[148, 33.6], [85, 26.6], [183, 23.3], [89, 28.1], [137, 43.1], [116, 25.6]])\n>>> y_small = np.array([1, 0, 0, 1, 1, 0])\n>>> \n>>> model = KNNClassifier(k=1, p=2)\n>>> model.fit(X_small, y_small)\n>>> \n>>> x_new_multiple = np.array([[110, 30.0], [148, 33.6], [85, 26.6]])\n>>> \n>>> np.array_equal(model.predict(x_new_multiple), np.array([0, 1, 0]))\nTrue",
         "failure_message": "KNNClassifier.predict returned the wrong labels for multiple points (example input from the prompt).",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> df = pd.read_csv('data/diabetes.csv')\n>>> X = df[['Glucose', 'BMI']].to_numpy()\n>>> y = df['Outcome'].to_numpy()\n>>> \n>>> model = KNNClassifier(k=13, p=2)\n>>> model.fit(X, y)\n>>> \n>>> np.isclose(model.predict(X).mean(), 0.300531914893617)\nTrue",
         "failure_message": "KNNClassifier.predict returned the wrong labels for many points (full dataset).",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> df = pd.read_csv('data/diabetes.csv')\n>>> X = df[['Glucose', 'BMI']].to_numpy()\n>>> y = df['Outcome'].to_numpy()\n>>> \n>>> model = KNNClassifier(k=5, p=1)\n>>> model.fit(X, y)\n>>> \n>>> np.isclose(model.predict(X).mean(), 0.3191489361702128)\nTrue",
         "failure_message": "KNNClassifier.predict returned the wrong labels for many points (full dataset).",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p06_e": {
     "name": "p06_e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> best_k in range(1, 51)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> best_k > 30\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> best_k in [34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
