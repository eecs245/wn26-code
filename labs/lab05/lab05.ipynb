{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0c2ab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab05.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8dd5b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "#### Lab 5\n",
    "    \n",
    "# Projections and Spans\n",
    "\n",
    "### EECS 245, Winter 2026 at the University of Michigan\n",
    "    \n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Most labs will have Jupyter Notebooks, like this one, designed to supplement the in-person worksheet. \n",
    "\n",
    "To write and run code in this notebook, you have two options:\n",
    "\n",
    "1. **Set up a Jupyter Notebook environment locally, and use `git` to clone our course repository (preferred)**. For instructions on how to do this, see the [**Environment Setup**](https://eecs245.org/env-setup) page of the course website. We have given you time in lab to follow these steps; if you have this notebook open, you probably already did them.\n",
    "1. **Use the EECS 245 DataHub.** To do this, click the \"code\" link under Lab 5 on the course website. Log in with your uniqname and set a password.\n",
    "\n",
    "To receive credit for the lab, you'll need to show your TA that all test cases have passed for all tasks before the end of the lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617041f",
   "metadata": {},
   "source": [
    "## From Words to Numbers ðŸ“•\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The big application we'll explore in the programming section of the lab is how to represent a **text** document as a **vectors**. In text analysis, each piece of text we want to analyze is called a **document**, and a collection of documents is called a **corpus**. \n",
    "\n",
    "For example, if we're analyzing the lyrics of different songs, each document might represent the lyrics to a single song, and our corpus would be the total set of lyrics we have access to.\n",
    "\n",
    "<center markdown=\"1\">\n",
    "<b>Goal: Use cosine similarity to measure the similarity between Presidential speeches.</b>\n",
    "</center>\n",
    "\n",
    "Each year, the sitting US President delivers a \"State of the Union\" address. The 2025 State of the Union (SOTU) address was on March 4th, 2025. (\"Address\" is another word for \"speech\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('XkFKNkAEzQ8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9f848",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The file `'data/stateoftheunion1790-2025.txt'` contains the transcript of every SOTU address since 1790. Go open it in your favorite text editor to see how it's formatted! (Source: [The American Presidency Project](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stateoftheunion1790-2025.txt') as f:\n",
    "    sotu = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acc109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus, in total, is over 10 million characters long!\n",
    "len(sotu) / 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610eb42d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below, we've provided code that converts `sotu`, a string with 10 million characters, to a `pandas` DataFrame object. `pandas` is a Python library designed to work with tabular data â€“ that is, data in tables â€“ and \"DataFrame\" is their name for tables.\n",
    "\n",
    "Run the next few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speeches are separated by ***.\n",
    "speeches_lst = sotu.split('\\n***\\n')[1:]\n",
    "len(speeches_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(speeches_lst[-1][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_speeches_df(speeches_lst):\n",
    "    def extract_struct(speech):\n",
    "        L = speech.strip().split('\\n', maxsplit=3)\n",
    "        L[3] = re.sub(r\"[^A-Za-z' ]\", ' ', L[3]).lower() # Replaces anything OTHER than letters with ' '.\n",
    "        L[3] = re.sub(r\"it's\", 'it is', L[3]).replace(' s ', '')\n",
    "        return dict(zip(['president', 'date', 'text'], L[1:]))\n",
    "\n",
    "    speeches = pd.DataFrame(list(map(extract_struct, speeches_lst)))\n",
    "    speeches.index = speeches['president'].str.strip() + ': ' + speeches['date']\n",
    "    speeches = speeches[['text']]\n",
    "    \n",
    "    return speeches\n",
    "\n",
    "speeches = create_speeches_df(speeches_lst)\n",
    "speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292b3da",
   "metadata": {},
   "source": [
    "Each row corresponds to a single speech; the DataFrame above has 235 rows, meaning we have the text of 235 speeches. Notice that punctuation has been removed from each speech, and all **terms** have been converted to lowercase.\n",
    "\n",
    "Our goal is to produce 235 vectors, \n",
    "\n",
    "$$\\vec v_{\\text{George Washington: January 8, 1790}}, \\vec v_{\\text{George Washington: December 8, 1790}}, \\ldots, \\vec v_{\\text{Donald J. Trump: March 4, 2025}}$$\n",
    "\n",
    "so that we can measure how similar any two speeches by computing the cosine similarity of their corresponding vectors. Make your predictions now: of all pairs of the 235 speeches, which will be the most similar? The least?\n",
    "\n",
    "### The Bag of Words Model\n",
    "\n",
    "The big question is **how** to represent a speech (or more generally, a document) as a vector. One simple idea is to count the number of occurrences of every term in every document, and store these counts in a vector.\n",
    "\n",
    "For example, consider the following 3 documents:\n",
    "\n",
    "1. **big big big big data class**\n",
    "1. **data big data science**\n",
    "1. **science big data**\n",
    "\n",
    "There are 4 unique terms across this corpus: **big**, **data**, **class**, and **science**. So, we can represent each document as a vector in $\\mathbb{R}^4$.\n",
    "\n",
    "$$\\vec v_i = \\begin{bmatrix} \\text{count of \"big\" in document } i \\\\ \\text{count of \"data\" in document } i \\\\ \\text{count of \"class\" in document } i \\\\ \\text{count of \"science\" in document } i \\end{bmatrix}$$\n",
    "\n",
    "For example, $\\vec v_1 = \\begin{bmatrix} 4 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$. See if you can identify what $\\vec v_2$ and $\\vec v_3$ are.\n",
    "\n",
    "This technique for representing documents as vectors of word counts is called the **bag of words** model. It's called this because it doesn't consider the order of the words in the document; imagine the words are separated by spaces, and are shuffled around in a bag. The order of them within a document is irrelevant; all we care about are their frequencies.\n",
    "\n",
    "<center><img src='imgs/bag-of-words.jpeg' width=500></center>\n",
    "\n",
    "In this toy example, we only have 3 vectors, so writing them out side-by-side is manageable. But in our speeches example, we have 235 speeches, so 235 vectors, and it'll become inconvenient to write them out this way. Instead, we can represent all of these vectors in a single table, where **each row corresponds to a document's vector representation**.\n",
    "\n",
    "| | big | data | class |  science |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| **big big big big data class** | 4 | 1 | 1 | 0 |\n",
    "| **data big data science** | 1 | 2 | 0 | 1 |\n",
    "| **science big data** | 1 | 1 | 0 | 1 |\n",
    "\n",
    "The first row of the table above is just $\\vec v_1$ from before.\n",
    "\n",
    "Once we have such a table, we can compute the cosine similarity between any pair of rows, which will give us the similarity between the corresponding documents.\n",
    "\n",
    "### Applying the Bag of Words Model to Presidential Speeches\n",
    "\n",
    "Let's produce this table for our speeches. We'll do some of the work for you, but you'll need to fill in the rest.\n",
    "\n",
    "First, we'll find **all** unique terms across all speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d560f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes each speech's text, splits by spaces to get a list of terms per speech, then explodes the list into a single list of terms.\n",
    "# The result below contains the unique terms, along with their counts across all speeches.\n",
    "# So, the word \"the\" appears 147744 times across all speeches.\n",
    "all_unique_terms = speeches['text'].str.split().explode().value_counts()\n",
    "all_unique_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f3349",
   "metadata": {},
   "source": [
    "Since there are over 20,000 unique terms â€“ 24,528 in fact â€“ our future calculations will otherwise take too much time to run. Let's take the 500 most frequent, across all speeches, for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms = list(all_unique_terms.iloc[:500].index)\n",
    "unique_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cb7d3",
   "metadata": {},
   "source": [
    "So, we'll represent each speech as a vector in $\\mathbb{R}^{500}$.\n",
    "\n",
    "Now, we need to find the number of occurrences of each term in each speech. For example, how many times does \"the\" appear in the first speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27298e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_speech = speeches['text'].iloc[0]\n",
    "first_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95edfe8",
   "metadata": {},
   "source": [
    "The string `.count` method should give us the number of times a substring appears in a string. Notice that we've counted for the number of occurrences of `\" the \"` rather than `\"the\"`, because we don't want to count instances of \"the\" that are part of other words, like \"thesaurus\" or \"there\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf019b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_speech.count(' the ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b3f9f",
   "metadata": {},
   "source": [
    "We _could_ write a nested loop, like:\n",
    "\n",
    "```\n",
    "for all 500 terms t:\n",
    "    for all 235 speeches d:\n",
    "        count the number of times t appears in d\n",
    "```\n",
    "\n",
    "But, as we've come to know in this course, there's _usually_ a better way. And indeed there is. `pandas` DataFrames, like `speeches` below, come equipped with several vectorized methods that allow us to apply an operation to every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878da53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce7688",
   "metadata": {},
   "source": [
    "Check out what happens below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches['text'].str.count(' the ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c697641",
   "metadata": {},
   "source": [
    "Above, we counted the number of times `\" the \"` appears in **each** speech. The first value above, 97, is the same as we got with `first_speech.count(' the ')`. This is considerably quicker than writing a loop over all 235 speeches.\n",
    "\n",
    "The data structure above is a `pandas` Series, which you can think of as a 1-dimensional array, along with an index, which is a name for each element in the array.\n",
    "\n",
    "So, the above allows us to instead write just a single loop:\n",
    "\n",
    "```\n",
    "for all 500 terms t:\n",
    "    count the number of times t appears in every document d, using .str.count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdcd27",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "### Task 1\n",
    "\n",
    "</div>\n",
    "\n",
    "Below, assign `counts_dict` to a **dictionary** with 500 keys, one for each of the top 500 unique terms. The value corresponding to a particular key should be a `pandas` Series of length 235, where each element is the number of times the key appears in a speech.\n",
    "\n",
    "For example, `counts_dict[\"the\"]` should be the same as `speeches['text'].str.count(\" the \")` from above.\n",
    "\n",
    "_Hint: Our solution uses a single `for`-loop, and takes ~10 seconds to run locally._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a4ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts_dict = ...\n",
    "\n",
    "# Feel free to change the string below to test your solution. \n",
    "counts_dict[\"americans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a06285",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"task01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bd716",
   "metadata": {},
   "source": [
    "Now that you've produced `counts_dict`, we can convert it to a DataFrame, where each row corresponds to a speech, and each column corresponds to a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame(counts_dict)\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b838de",
   "metadata": {},
   "source": [
    "Each **row** is a vector in $\\mathbb{R}^{500}$, corresponding to a different speech. The speech names on the left are the **index** of the DataFrame, and aren't included as part of the vector.\n",
    "\n",
    "To access the vector for a particular speech, we can use the `.loc` accessor. For example, `counts_df.loc[\"George Washington: January 8, 1790\"]` gives us the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.loc[\"George Washington: January 8, 1790\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a42d92",
   "metadata": {},
   "source": [
    "Equivalently, to access row `i` (indexed starting at 0), we can use `counts_df.iloc[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc557f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above!\n",
    "counts_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8390e7",
   "metadata": {},
   "source": [
    "Now that we know how to access the vector for a particular speech, we can compute the cosine similarity between any two speeches!\n",
    "\n",
    "First, let's implement a general-purpose cosine similarity function that works on any two vectors $\\vec u, \\vec v \\in \\mathbb{R}^n$. Before you complete the next task, review the [end of Chapter 3.2](https://notes.eecs245.org/vectors/norms/#np-linalg-norm-and-vectorization) and the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.dot([1, 2], [3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23ca34",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "### Task 2\n",
    "\n",
    "</div>\n",
    "\n",
    "Complete the implementation of the function `cosine_similarity`, which takes in two lists, arrays, or Series `u` and `v`, both corresponding to vectors in $\\mathbb{R}^n$, and returns their cosine similarity. Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> cosine_similarity([4, 3, 2], [1, -1, 1])\n",
    "0.3216337604513385\n",
    "\n",
    ">>> cosine_similarity(counts_df.iloc[0], counts_df.iloc[1])\n",
    "0.9678045752893217\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce9a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    # Converts u and v to numpy arrays, in case they're lists or pandas Series to begin with.\n",
    "    u = np.array(u)\n",
    "    v = np.array(v)\n",
    "    ...\n",
    "\n",
    "# Feel free to change the inputs below to test your solution.\n",
    "cosine_similarity([4, 3, 2], [1, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d785a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"task02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbf204",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "### Task 3\n",
    "\n",
    "</div>\n",
    "\n",
    "Now, complete the implementation of the function `similarity_of_rows`, which takes in a DataFrame `df` and  **strings** `index_1` and `index_2` and returns the cosine similarity of the rows of `df` with index values of `index_1` and `index_2`, respectively.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> similarity_of_speeches(counts_df, \n",
    "                           'Benjamin Harrison: December 3, 1889', \n",
    "                           'George H.W. Bush: January 28, 1992')\n",
    "0.8487541155585746\n",
    "```\n",
    "\n",
    "_Hint: Our implementation is only one line long. Make sure `counts_df` doesn't appear in your solution! We've defined this function this way so that later on, `df` could be a **different** table with alternative vector representations of each speech._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0588b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def similarity_of_speeches(df, index_1, index_2):\n",
    "    ...\n",
    "    \n",
    "# Feel free to change the inputs below to test your solution.\n",
    "similarity_of_speeches(counts_df, \n",
    "                       'Benjamin Harrison: December 3, 1889', \n",
    "                       'George H.W. Bush: January 28, 1992')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e314d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"task03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7862cfa",
   "metadata": {},
   "source": [
    "Let's use your implementation of `similarity_of_speeches` to compute the cosine similarity between all $\\frac{n(n-1)}{2}$ pairs of speeches. (There are $n \\choose 2$ pairs, if the binomial coefficient sounds familiar from EECS 203.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7197315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "sims_dict = {}\n",
    "# For every pair of speeches, find the similarity and store it in\n",
    "# the sims_dict dictionary.\n",
    "for pair in combinations(counts_df.index, 2):\n",
    "    sims_dict[pair] = similarity_of_speeches(counts_df, pair[0], pair[1])\n",
    "    \n",
    "# Turn the sims_dict dictionary into a DataFrame.\n",
    "sims = (\n",
    "    pd.Series(sims_dict)\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_0': 'speech 1', 'level_1': 'speech 2', 0: 'cosine similarity'})\n",
    "    .sort_values('cosine similarity', ascending=False)\n",
    ")\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d2362",
   "metadata": {},
   "source": [
    "It seems that the most similar pair of speeches, when we use the bag of words model to convert speeches to vectors, are speeches by William H. Taft in 1909 and 1911. The most dissimilar pair is a speech by John Quincy Adams from 1827 and George W. Bush in 2001. Cool!\n",
    "\n",
    "Throughout this lab, you may have realized there's a key flaw with the bag of words model. Words like \"the\" and \"of\" appear very frequently in every speech, and so the \"the\" and \"of\" components of our vectors in $\\mathbb{R}^{500}$ will consistently be very large. This will cause the cosine similarities of most pairs of vectors to be relatively large, not because they use a similar combination of rare-ish words, but rather, because they both use lots of the same common words.\n",
    "\n",
    "You should notice in the table above in the `cosine similarity` column that even the least similar pair has a relatively high cosine similarity of 0.713541.\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Instead of using the bag of words model to convert speeches to vectors, let's instead use a different metric, called the **term frequency-inverse document frequency**.\n",
    "\n",
    "Suppose $t$ is a single term and $d$ is a document. Then:\n",
    "\n",
    "$$\\begin{align*}\\text{tfidf}(t, d) &= \\frac{\\text{\\# of occurrences of $t$ in $d$}}{\\text{total \\# of terms in $d$}} \\cdot \\log \\left(\\frac{\\text{total \\# of documents}}{\\text{\\# of documents in which $t$ appears}} \\right)\\end{align*} $$\n",
    "\n",
    "$\\text{tfidf}(t, d)$ is large when $t$ is common in document $d$, but rare overall, so you can think of it as an \"importance score\" for $t$ in $d$.\n",
    "\n",
    "Before we apply this to our speeches dataset, let's work through a toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2449a92",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "### Task 4\n",
    "\n",
    "</div>\n",
    "\n",
    "Consider the following three documents.\n",
    "\n",
    "1. **big big big big data class**\n",
    "1. **data big data science**\n",
    "1. **science big data**\n",
    "\n",
    "Assign `tfidf_science_doc2` to $\\text{tfidf}(\\text{\"science\", document 2})$ and `tfidf_big_doc1` to $\\text{tfidf}(\\text{\"big\", document 1})$. A related value has already been calculated for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843bac67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_class_doc3 = (0 / 3) * np.log(3 / 1)\n",
    "tfidf_science_doc2 = ...\n",
    "tfidf_big_doc1 = ...\n",
    "\n",
    "print('tfidf(\"class\", document 3) = ', tfidf_class_doc3)\n",
    "print('tfidf(\"science\", document 2) = ', tfidf_science_doc2)\n",
    "print('tfidf(\"big\", document 1) = ', tfidf_big_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee2a12",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"task04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713eef27",
   "metadata": {},
   "source": [
    "You'll notice that both `tfidf_class_doc3` and `tfidf_big_doc1` were 0. But, they were 0 for different reasons.\n",
    "\n",
    "Recall,\n",
    "\n",
    "$$\\begin{align*}\\text{tfidf}(t, d) &= \\frac{\\text{\\# of occurrences of $t$ in $d$}}{\\text{total \\# of terms in $d$}} \\cdot \\log \\left(\\frac{\\text{total \\# of documents}}{\\text{\\# of documents in which $t$ appears}} \\right)\\end{align*} $$\n",
    "\n",
    "\n",
    "$\\text{tfidf}(t, d)$ is 0 when:\n",
    "- $t$ doesn't appear in $d$. Here, $\\frac{\\text{\\# of occurrences of $t$ in $d$}}{\\text{total \\# of terms in $d$}} = 0$.\n",
    "- $t$ appears in every single document. Why does this imply $\\text{tfidf}(t, d) = 0$?\n",
    "\n",
    "The hope, in introducing TF-IDF, is that if we use TF-IDF scores to turn speeches into vectors, the resulting vectors will contain more meaningful information about each speech, rather than just raw word frequencies.\n",
    "\n",
    "Somehow, we'll need to create the equivalent of `counts_df`, but at row $d$ and column $t$, the new table should contain $\\text{tfidf}(t, d)$, rather than $\\text{count}(t, d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8023d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb65fa",
   "metadata": {},
   "source": [
    "You've now finished the required programming component of Lab 5.\n",
    "\n",
    "**Open-ended challenge**: In the space below, we encourage you to _try_ and figure out how to create a table `tfidf_df`, like the one we describe above.\n",
    "\n",
    "There are a few ways to go about it:\n",
    "- Start with `counts_df`, and use it to compute term frequencies (the TF in TF-IDF) and inverse document frequencies.\n",
    "- Use the `TfidfVectorizer` class from `sklearn.feature_extraction.text`. Once you instantiate a `TfidfVectorizer` object, use the `fit_transform` method on it.\n",
    "\n",
    "Feel free to come to us in office hours if you'd like guidance on how to make this work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2d77a",
   "metadata": {},
   "source": [
    "## Finish Line ðŸ\n",
    "\n",
    "You're ready to have the programming portion of Lab 5 checked off by your TA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34cd5b9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a8079",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "otter": {
   "tests": {
    "task01": {
     "name": "task01",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(counts_dict, dict) and len(counts_dict) == 500\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(counts_dict[\"the\"], pd.Series) and len(counts_dict[\"the\"]) == 235\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> counts_dict.keys()\ndict_keys(['the', 'of', 'and', 'to', 'in', 'a', 'that', 'for', 'be', 'our', 'is', 'it', 'by', 'we', 'have', 'this', 'as', 'with', 'which', 'i', 'will', 'on', 'are', 'not', 'has', 'been', 'their', 'from', 'all', 'at', 'government', 'states', 'an', 'or', 'but', 'was', 'its', 'they', 'should', 'congress', 'united', 'more', 'can', 'these', 'people', 'so', 'such', 'year', 'upon', 'would', 'them', 'other', 'no', 'country', 'than', 'may', 'must', 'great', 'any', 'now', 'who', 'those', 'made', 'new', 'public', 'there', 'were', 'under', 'one', 'if', 'american', 'time', 'war', 'you', 'last', 'us', 'my', 'world', 'years', 'his', 'only', 'every', 'most', 'had', 'into', 'state', 'do', 'some', 'national', 'law', 'when', 'power', 'present', 'between', 'make', 'peace', 'nation', 'shall', 'act', 'also', 'work', 'own', 'citizens', 'nations', 'many', 'general', 'america', 'without', 'against', 'well', 'during', 'system', 'service', 'first', 'over', 'what', 'part', 'two', 'foreign', 'necessary', 'good', 'out', 'he', 'federal', 'same', 'before', 'much', 'subject', 'your', 'interest', 'very', 'just', 'policy', 'department', 'laws', 'through', 'men', 'while', 'both', 'legislation', 'free', 'treaty', 'right', 'long', 'important', 'being', 'up', 'within', 'about', 'increase', 'could', 'business', 'trade', 'duty', 'because', 'union', 'need', 'interests', 'security', 'president', 'commerce', 'means', 'where', 'military', 'rights', 'future', 'action', 'far', 'large', 'each', 'still', 'treasury', 'attention', 'after', 'americans', 'way', 'countries', 'report', 'give', 'since', 'progress', 'purpose', 'me', 'economic', 'consideration', 'force', 'administration', 'past', 'secretary', 'th', 'relations', 'already', 'constitution', 'amount', 'among', 'take', 'yet', 'economy', 'even', 'her', 'help', 'whole', 'tax', 'powers', 'use', 'best', 'continue', 'further', 'less', 'day', 'mexico', 'governments', 'officers', 'hope', 'am', 'ever', 'however', 'support', 'high', 'life', 'condition', 'fiscal', 'know', 'here', 'thus', 'labor', 'number', 'justice', 'expenditures', 'given', 'might', 'better', 'international', 'together', 'authority', 'full', 'commission', 'session', 'order', 'like', 'duties', 'home', 'navy', 'possible', 'period', 'question', 'executive', 'too', 'program', 'freedom', 'proper', 'effect', 'never', 'done', 'taken', 'toward', 'land', 'revenue', 'debt', 'territory', 'come', 'place', 'defense', 'increased', 'meet', 'health', 'protection', 'money', 'measures', 'again', 'care', 'control', 'believe', 'recommend', 'themselves', 'course', 'several', 'provide', 'result', 'army', 'making', 'next', 'certain', 'then', 'end', 'million', 'having', 'political', 'private', 'become', 'whether', 'view', 'therefore', 'plan', 'tonight', 'three', 'secure', 'pay', 'prosperity', 'another', 'lands', 'commercial', 'development', 'conditions', 'claims', 'office', 'others', 'children', 'character', 'forces', 'effort', 'spirit', 'jobs', 'fact', 'history', 'special', 'vessels', 'resources', 'budget', 'greater', 'millions', 'common', 'importance', 'existing', 'senate', 'change', 'small', 'until', 'let', 'due', 'opportunity', 'received', 'republic', 'education', 'itself', 'want', 'efforts', 'civil', 'cost', 'value', 'established', 'regard', 'representatives', 'down', 'property', 'found', 'house', 'provision', 'capital', 'message', 'construction', 'june', 'him', 'man', 'matter', 'always', 'put', 'how', 'respect', 'needs', 'energy', 'convention', 'say', 'strength', 'case', 'few', 'per', 'cause', 'today', 'see', 'either', 'local', 'growth', 'proposed', 'whose', 'nor', 'required', 'protect', 'social', 'dollars', 'billion', 'provided', 'industry', 'keep', 'members', 'aid', 'europe', 'able', 'strong', 'equal', 'go', 'production', 'annual', 'nearly', 'get', 'early', 'affairs', 'working', 'bring', 'programs', 'britain', 'principles', 'does', 'including', 'paid', 'ago', 'cases', 'especially', 'reason', 'brought', 'community', 'called', 'appropriations', 'families', 'responsibility', 'once', 'establishment', 'results', 'british', 'welfare', 'st', 'additional', 'measure', 'domestic', 'provisions', 'line', 'institutions', 'back', 'naval', 'passed', 'minister', 'court', 'form', 'different', 'ought', 'true', 'individual', 'said', 'ask', 'human', 'indian', 'prevent', 'total', 'reduction', 'bill', 'cannot', 'continued', 'rate', 'spain', 'necessity', 'soon', 'price', 'percent', 'permanent', 'effective', 'four', 'times', 'maintain', 'essential', 'reform', 'require', 'receipts', 'problems', 'sum', 'pacific', 'did', 'friendly', 'tariff', 'experience', 'information', 'purposes', 'self', 'held', 'fully', 'opinion', 'population', 'adequate', 'parties', 'banks', 'income', 'fellow', 'extent', 'success', 'fair', 'principle', 'rates', 'settlement', 'point', 'improvement', 'months', 'abroad', 'demand', 'call', 'old', 'think', 'throughout', 'increasing', 'desire', 'submitted', 'building'])",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(unique_terms) == set(counts_dict.keys())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (counts_dict[\"the\"] == speeches['text'].str.count(\" the \")).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (counts_dict[\"americans\"] == speeches['text'].str.count(\" americans \")).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (counts_dict[\"support\"] == speeches['text'].str.count(\" support \")).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> counts_dict[\"jobs\"].sum() == 620\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task02": {
     "name": "task02",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(cosine_similarity([1, 2], [3, 4]), 11 / (np.sqrt(5) * 5))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(cosine_similarity([4, 3, 2], [1, -1, 1]), 0.3216337604513385)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(cosine_similarity(counts_df.iloc[0], counts_df.iloc[1]), 0.9678045752893217)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(cosine_similarity(counts_df.iloc[-3], counts_df.iloc[-150]), 0.8786296721466733)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task03": {
     "name": "task03",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(similarity_of_speeches(counts_df, 'Benjamin Harrison: December 3, 1889', 'George H.W. Bush: January 28, 1992'), 0.8487541155585746)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(similarity_of_speeches(counts_df, counts_df.index[6], counts_df.index[5]), 0.9641631399675707)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(similarity_of_speeches(counts_df.tail(5), 'Joseph R. Biden Jr.: February 7, 2023', 'Joseph R. Biden Jr.: March 7, 2024'), 0.9764510999270145)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task04": {
     "name": "task04",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(tfidf_science_doc2, 0.1013662770270411)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(tfidf_big_doc1, 0)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
